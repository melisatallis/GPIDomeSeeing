{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aotelem\n",
    "This notebook pulls down data for raw IFS frames. Only images that were successfully processed to \"reduced data products\" are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import time\n",
    "import astropy.time as aptime\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import gpiTools as gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pylab.rcParams['figure.figsize'] = (14.0, 5.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query DB. Seeing entries will need cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querydbflag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if querydbflag:\n",
    "    import gpifilesdb\n",
    "    db = gpifilesdb.GPIFilesDB(server='127.0.0.1', username='mtallis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains data that matches up with AO telemetry. Used to calibrate the WFE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting dataframe to /Users/MelisaT/Dropbox (GPI)/TEST_SCRATCH/scratch/mtallis/code/\n",
      "There are 2009 entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'dbres' in locals():\n",
    "    del(dbres)\n",
    "        \n",
    "savedir = os.environ[\"HOME\"]+'/Dropbox (GPI)/TEST_SCRATCH/scratch/mtallis/code/'\n",
    "\n",
    "if querydbflag:\n",
    "    query = \"\"\"SELECT\n",
    "    AORAW_FILES.datafile, objname,whenstr from AORAW\n",
    "    left join AORAW_FILES on AORAW.UID = AORAW_FILES.RAWID\n",
    "    where objname not like 'Zenith' and\n",
    "    aoframes like 1000 and\n",
    "    datafile like '%_phase%' order by whenstr\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # the directory stuff at the end is b/c there are a \n",
    "    # couple duplicate folders with permutations of the same name\n",
    "    # and the data got stored in both (eg: HD 75519 and HD_75519)\n",
    "    # Get rid of the non-standard directory names\n",
    "\n",
    "\n",
    "    dbres = db._do_query(query)\n",
    "    dbdf = pd.DataFrame(dbres)\n",
    "    \n",
    "    print 'outputting dataframe to '+savedir\n",
    "    dstr = time.strftime('%Y%m%d')\n",
    "    prefix = 'aotelem_'\n",
    "    dbdf.to_excel(savedir+prefix+dstr+'.xls', index=False)\n",
    "    dbdf.to_csv(savedir+prefix+dstr+'.txt', index=False)\n",
    "\n",
    "    \n",
    "else:\n",
    "    dstr = '20190226'\n",
    "    print \"******************************************************************\"\n",
    "    print \"Loading DB query w/ dirty seeing queried on \"+dstr\n",
    "    print \"******************************************************************\"\n",
    "\n",
    "    dbdf = pd.read_csv(savedir+'aotelem_'+dstr+'.txt')\n",
    "    \n",
    "print \"There are %g entries.\\n\"%len(dbdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not link to the AO telemetry data. Only contains IFS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting dataframe to /Users/MelisaT/Dropbox (GPI)/TEST_SCRATCH/scratch/mtallis/code/\n",
      "There are 27611 entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'dbres' in locals():\n",
    "    del(dbres)\n",
    "        \n",
    "savedir = os.environ[\"HOME\"]+'/Dropbox (GPI)/TEST_SCRATCH/scratch/mtallis/code/'\n",
    "\n",
    "if querydbflag:\n",
    "    query = \"\"\"SELECT\n",
    "    RawDataProducts.DATAFILE, DATALAB, OBJNAME,\n",
    "    DATESTR, UTSTART, MJDOBS, ITIME, COADDS,\n",
    "    OBSMODE, DISPERSR, IFSFILT, AOFRAMES, AOSPATIA,\n",
    "    HMAG, STMAG as IMAG, FLUX as AOFLUX, AOWFE as RawDPwfe, \n",
    "    \n",
    "    PAR_ANG, PA, IAA, AZIMUTH, ELEVATIO, AIRMASS, AMSTART, AMEND,\n",
    "    WINDM2, WINDM2DR, WINDDIRE, WINDSPEE, TAMBIENT, OMSATEMP, GLITEMP, GLOTEMP,\n",
    "    MASSSEE, MASSTAU, DIMMSEE,\n",
    "    MASS05CN, MASS1CN2, MASS2CN2, MASS4CN2, MASS8CN2, MASS16CN, MASSISOP,    \n",
    "    \n",
    "    DRPDATE,\n",
    "    ReducedDataProducts.CONTR025, ReducedDataProducts.CONTR040, ReducedDataProducts.CONTR080\n",
    "    \n",
    "    from RawDataProducts\n",
    "    left join OBSNOTES on RawDataProducts.UID = OBSNOTES.UID\n",
    "    inner join Raw2Reduced on RawDataProducts.UID = Raw2Reduced.RAWID\n",
    "    inner join ReducedDataProducts on Raw2Reduced.REDID = ReducedDataProducts.UID\n",
    "    \n",
    "    where RawDataProducts.OBJNAME not like 'Zenith'\n",
    "    and RawDataProducts.ARTSRC like 'GPI_DEPLOY_OUT'\n",
    "    and RawDataProducts.AOCLOOP like 'TRUE'\n",
    "    and RawDataProducts.DATAFILE like 'S201%S%.fits'\n",
    "    and OBSNOTES.markedbad is NULL\n",
    "\n",
    "    and ReducedDataProducts.DATAFILE like 'S201%S%_spdc_distorcorr.fits'\n",
    "    and ReducedDataProducts.DROPBOXPATH like '%autoreduced%'\n",
    "    and ReducedDataProducts.DROPBOXPATH not like '%Non-Campaign%'\n",
    "    and ReducedDataProducts.filetype = 'Spectral Cube'\n",
    "\n",
    "    and ReducedDataProducts.DROPBOXPATH not like '%/HR_4796/%'\n",
    "    and ReducedDataProducts.DROPBOXPATH not like '%/HD 75519/%'\n",
    "    \n",
    "    order by RawDataProducts.DATAFILE , DRPDATE DESC    \n",
    "    \"\"\"\n",
    "\n",
    "    # the directory stuff at the end is b/c there are a \n",
    "    # couple duplicate folders with permutations of the same name\n",
    "    # and the data got stored in both (eg: HD 75519 and HD_75519)\n",
    "    # Get rid of the non-standard directory names\n",
    "\n",
    "\n",
    "    dbres = db._do_query(query)\n",
    "    dbdf = pd.DataFrame(dbres)\n",
    "    del(dbres)\n",
    "    dbdf['MJDOBS'] = pd.to_numeric(dbdf['MJDOBS'])\n",
    "    \n",
    "    print 'outputting dataframe to '+savedir\n",
    "    dstr = time.strftime('%Y%m%d')\n",
    "    prefix = 'IFS_AllOnSky_RawDistorcorr_'\n",
    "    dbdf.to_excel(savedir+prefix+dstr+'.xls', index=False)\n",
    "    dbdf.to_csv(savedir+prefix+dstr+'.txt', index=False)\n",
    "\n",
    "    \n",
    "else:\n",
    "    dstr = '20190226'\n",
    "    print \"******************************************************************\"\n",
    "    print \"Loading DB query w/ dirty seeing queried on \"+dstr\n",
    "    print \"******************************************************************\"\n",
    "\n",
    "    dbdf = pd.read_csv(savedir+'IFS_AllOnSky_RawDistorcorr_'+dstr+'.txt')\n",
    "    \n",
    "print \"There are %g entries.\\n\"%len(dbdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFILE     object\n",
      "DATALAB      object\n",
      "OBJNAME      object\n",
      "DATESTR      object\n",
      "UTSTART      object\n",
      "MJDOBS      float64\n",
      "ITIME       float64\n",
      "COADDS        int32\n",
      "OBSMODE      object\n",
      "DISPERSR     object\n",
      "IFSFILT      object\n",
      "AOFRAMES      int32\n",
      "AOSPATIA    float64\n",
      "HMAG        float64\n",
      "IMAG        float64\n",
      "AOFLUX      float64\n",
      "RawDPwfe    float64\n",
      "PAR_ANG     float64\n",
      "PA          float64\n",
      "IAA         float64\n",
      "AZIMUTH     float64\n",
      "ELEVATIO    float64\n",
      "AIRMASS     float64\n",
      "AMSTART     float64\n",
      "AMEND       float64\n",
      "WINDM2      float64\n",
      "WINDM2DR    float64\n",
      "WINDDIRE    float64\n",
      "WINDSPEE    float64\n",
      "TAMBIENT    float64\n",
      "OMSATEMP    float64\n",
      "GLITEMP     float64\n",
      "GLOTEMP     float64\n",
      "MASSSEE     float64\n",
      "MASSTAU     float64\n",
      "DIMMSEE     float64\n",
      "MASS05CN    float64\n",
      "MASS1CN2    float64\n",
      "MASS2CN2    float64\n",
      "MASS4CN2    float64\n",
      "MASS8CN2    float64\n",
      "MASS16CN    float64\n",
      "MASSISOP    float64\n",
      "DRPDATE      object\n",
      "CONTR025    float64\n",
      "CONTR040    float64\n",
      "CONTR080    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print dbdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for multiply-matched raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files = 27611\n",
      "number of unique= 27606\n",
      "                  DATAFILE  CONTR040          OBJNAME\n",
      "24788  S20180811S0166.fits       NaN  TYC 8785-1322-1\n",
      "24807  S20180811S0185.fits  0.000270  TYC 8785-1322-1\n",
      "24843  S20180811S0223.fits  0.000123           BO Mic\n",
      "24845  S20180811S0224.fits  0.000134           BO Mic\n",
      "24870  S20180811S0251.fits  0.000072           mu PsA\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Duplicate raw entries. You figure out what to do.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-39724c2ffaa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATAFILE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CONTR040'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'OBJNAME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Duplicate raw entries. You figure out what to do.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Duplicate raw entries. You figure out what to do."
     ]
    }
   ],
   "source": [
    "if len(set(dbdf['DATAFILE'])) < len(dbdf):\n",
    "    print 'number of files = '+str(len(dbdf))\n",
    "    print 'number of unique= '+str(len(set(dbdf['DATAFILE'])))\n",
    "    #raise Exception('Some raw files have multiple entries. Quitting so you can figure this out.')\n",
    "    \n",
    "    f0 = ''\n",
    "    ct = 0\n",
    "    df1 = pd.DataFrame(dbdf[ct:ct+1], index=[0])\n",
    "\n",
    "    for f in dbdf['DATAFILE']:\n",
    "        if f == f0:\n",
    "            df1 = df1.append(dbdf[ct:ct+1], ignore_index=False)\n",
    "        f0 = f\n",
    "        ct = ct+1\n",
    "\n",
    "    df1.drop(0,inplace=True) # because it was a dummy entry\n",
    "    \n",
    "    print df1[['DATAFILE','CONTR040','OBJNAME']]\n",
    "    \n",
    "    raise Exception('Duplicate raw entries. You figure out what to do.')\n",
    "\n",
    "else:\n",
    "    print 'Hurray! All raw files have only 1 reduced match.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bad seeing values \n",
    "- 0\n",
    "- repeated for >=30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember: in DATESTR we use the UT date that covers most\n",
    "#  of the night (everthing from ~3pm local time on is recorded with the next date)\n",
    "# the \"datetimes\" reconstruction from MJD is the right one!\n",
    "\n",
    "t = aptime.Time(dbdf['MJDOBS'], format='mjd')\n",
    "#dbdf['datetime'] = t.datetime\n",
    "#dbdf[['DATESTR','UTSTART','datetime']][270:280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeing_vars = ['DIMMSEE','MASSSEE','MASSTAU','MASSISOP','MASS05CN',\\\n",
    "               'MASS1CN2','MASS2CN2','MASS4CN2','MASS4CN2','MASS8CN2','MASS16CN']\n",
    "seeing_is_invalid = {}\n",
    "for var in seeing_vars:\n",
    "    # remove repeats\n",
    "    seeing_is_invalid[var] = gpt.find_invalid_repeats(dbdf[var],t.datetime,30)\n",
    "    dbdf.loc[seeing_is_invalid[var],var] = np.nan\n",
    "    \n",
    "    # remove 0s, if present\n",
    "    try:\n",
    "        dbdf.loc[dbdf[var]==0, var] = np.nan\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbdf = dbdf.drop_duplicates(subset='DATAFILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savedir = os.environ[\"HOME\"]+'/Dropbox (GPI)/TEST_SCRATCH/scratch/mtallis/code/'\n",
    "prefix = 'IFS_AllOnSky_RawDistorcorr_CleanSee30_'\n",
    "#prefix = 'AO_telem_'\n",
    "dbdf.to_excel(savedir+prefix+dstr+'.xls', index=False)\n",
    "dbdf.to_csv(savedir+prefix+dstr+'.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
