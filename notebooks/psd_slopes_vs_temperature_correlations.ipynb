{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.fftpack as fft\n",
    "from astropy.io import fits\n",
    "from scipy import optimize\n",
    "from poppy import zernike\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "import os\n",
    "import pytz\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telescope dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outD = 7.77010            # primary diameter (m)\n",
    "inD = 1.024               # inner M2 diameter (m)\n",
    "n = 48                    # number sample points across the screen (Not the number of subapertures)\n",
    "nacross = 43              # number of subapertures across the aperture\n",
    "pscale = outD/(nacross)   # pixel size (m) of samples in pupil plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Aperture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n"
     ]
    }
   ],
   "source": [
    "#  Aperture containing zeros\n",
    "x = np.linspace(-(n)/2,(n)/2,n)*pscale \n",
    "y = np.linspace(-(n)/2,(n)/2,n)*pscale\n",
    "mg = np.meshgrid(x,y)\n",
    "ar = np.sqrt(np.sum((m**2 for m in mg)))\n",
    "ap_outer = (ar <= (7.3)/2) #mask is slightly oversized because GPI does not correct the boundaries well\n",
    "ap_inner = (ar <= 1.5/2)   \n",
    "\n",
    "#ap_outer = (ar <= outD/2)\n",
    "#ap_inner = (ar <= inD/2)   \n",
    "ap = (ap_outer ^ ap_inner).astype(np.float)\n",
    "\n",
    "#  Aperture containing nans\n",
    "ap_nan = np.copy(ap.astype(np.float))  \n",
    "ap_nan[np.where(ap==0)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make spatial frequency grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n"
     ]
    }
   ],
   "source": [
    "kx = fft.fftshift(fft.fftfreq(n,pscale))\n",
    "ky = fft.fftshift(fft.fftfreq(n,pscale))\n",
    "mg = np.meshgrid(kx,ky)\n",
    "kr = np.sqrt(np.sum((m**2 for m in mg))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. name_list - list containing telemetry date strings\n",
    "2. fname_list - list containing path to telemetry files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path_manager(desktop_name):\n",
    "    \n",
    "    if desktop_name == 'gpi_cruncher':\n",
    "        rootdir = '/home/sda/mtallis/PhaseScripts/aotelem/Reduced/'\n",
    "        save_path = '/home/sda/mtallis/Results/c_Eri/'\n",
    "        samples_path = '/home/sda/mtallis/samples/c_Eri_samples.txt'\n",
    "    \n",
    "    if desktop_name == 'kipac':\n",
    "        rootdir = '/Users/MelisaT/Documents/Research/GPIDomeSeeing/data/Results/c_Eri/'\n",
    "        save_path = '/Users/MelisaT/Documents/Research/GPIDomeSeeing/data/Results/c_Eri/'\n",
    "        samples_path = '/Users/MelisaT/Documents/Research/GPIDomeSeeing/data/datatables/c_Eri_samples.txt'\n",
    "        \n",
    "    if desktop_name == 'laptop':\n",
    "        rootdir = '/Users/melisatallis/Documents/Research/GPIDomeSeeing/data/Results/c_Eri/'\n",
    "        save_path = '/Users/melisatallis/Documents/Research/GPIDomeSeeing/data/Results/c_Eri/'\n",
    "        samples_path = '/Users/melisatallis/Documents/Research/GPIDomeSeeing/data/Results/c_Eri/c_Eri_samples.txt'\n",
    "\n",
    "    dstr = time.strftime('%Y%m%d')\n",
    "    return rootdir,save_path, samples_path, dstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootdir, save_path, samples_path, dstr = path_manager('gpi_cruncher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(samples_path,'r') as f:\n",
    "    sample = f.read().splitlines() #outputs as a list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use when fits files are available, which are typically stored in GPI cruncher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_list = list()\n",
    "name_list = list()\n",
    "\n",
    "for i in sample:\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for name in files:\n",
    "            (base,ext) = os.path.splitext(name)\n",
    "            if (ext in ('.fits')) and (i in base):\n",
    "                full_name = os.path.join(root,name)\n",
    "                #print(full_name)\n",
    "                fname_list.append(full_name)  \n",
    "                name_list.append(base[11:-12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. processs_phase - Imports .fits & removes static aberrations in each phase  \n",
    "2. radialProfile - Computes mean inside each radial bin starting from center\n",
    "3. sp_power_spec - Computes time average of 2d DFT^2  \n",
    "4. temp_power_spec - Computes actuator timeseries DFT^2 and averages over aperture\n",
    "5. linear fit - fits a power law to psd. Behaves like a line in loglog space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_phase(filepath):\n",
    "\n",
    "    hdulist = fits.open(filepath,memmap=True)\n",
    "    phase = hdulist[0].data.astype('float')\n",
    "    avg_phase = np.nanmean(phase*ap_nan,axis=0)  # used to find average zernikes \n",
    "\n",
    "    # remove zernikes form cube\n",
    "    z_basis = zernike.zernike_basis_faster(nterms= 6, npix = 48)\n",
    "    z_coeff = zernike.opd_expand_nonorthonormal(avg_phase, aperture=ap, nterms=6)\n",
    "    thin_lens = np.sum(z_coeff[:,None,None]*z_basis[:,:,:],axis=0)\n",
    "\n",
    "    c_phase = (phase - thin_lens[None,:,:])*ap_nan\n",
    "    c_phase[np.isnan(c_phase)]=0.\n",
    "    \n",
    "    return c_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radialProfile(image, center=None):\n",
    "    \"\"\"\n",
    "    Calculate the avearge radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Calculate the indices from the image\n",
    "    y,x = np.indices((image.shape)) # first determine radii of all pixels\n",
    "    \n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "     \n",
    "    r = np.hypot(x - center[0], y - center[1]).astype(np.int) \n",
    "    \n",
    "    n = np.bincount(r.ravel())\n",
    "    sy = np.bincount(r.ravel(), image.ravel())\n",
    "    mean = sy/n\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sp_power_spec(phase):    \n",
    "    \n",
    "    timesteps, phx, phy = phase.shape \n",
    "    phFT = np.zeros((timesteps,phx,phy), dtype=complex)\n",
    "    for t in np.arange(timesteps):\n",
    "        phFT[t,:,:] = fft.fftshift(fft.fft2(phase[t,:,:]))*2.0/ap.sum()\n",
    "    print('Done with FT')\n",
    "    \n",
    "    # compute 2d psd cube\n",
    "    psd2D = np.zeros((timesteps, phx, phy),dtype=float)\n",
    "    for k in np.arange(phx):\n",
    "        for l in np.arange(phy):\n",
    "            psd2D[:,k,l] = np.square(np.abs(phFT[:,k,l]))\n",
    "    \n",
    "    avg_psd2D = np.mean(psd2D, axis=0)\n",
    "    print('Done with PSD')    \n",
    "    \n",
    "    # compute radial average of 2d psd cube and frequency\n",
    "    avg_psd1D =  radialProfile(avg_psd2D)\n",
    "    \n",
    "    return avg_psd1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temp_power_spec(Y):\n",
    "    \n",
    "    n = len(Y)\n",
    "    w = signal.blackman(n)\n",
    "    P = np.fft.rfft(Y*w)\n",
    "    norm = 1000.0/n\n",
    "    P = P * norm\n",
    "    P2 = np.square(np.abs(P))\n",
    "    \n",
    "    return P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_fit(k,Y,low_b,up_b):\n",
    "\n",
    "    par = np.polyfit(np.log10(k[(k>low_b) & (k<up_b)]), np.log10(Y[(k>low_b) & (k<up_b)]), 1)\n",
    "    slope = par[0]\n",
    "    intercept = par[1]\n",
    "    \n",
    "    return slope, intercept "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define spatial frequncy grid - has 34 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = radialProfile(kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store spatial PSD values in DataFrame for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sp_psd = pd.DataFrame(columns = name_list)\n",
    "\n",
    "i=0\n",
    "for file in fname_list:\n",
    "    print(file)\n",
    "    print(name_list[i])\n",
    "    y_2D = process_phase(file)\n",
    "    y_psd = sp_power_spec(y_2D)\n",
    "    df_sp_psd[name_list[i]] = y_psd\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sp_psd.to_csv(save_path+'sp_psd'+'_'+dstr+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store temporal PSD values in dataframe for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_t_psd = pd.DataFrame()\n",
    "\n",
    "i=0\n",
    "for file in fname_list[0:2]:\n",
    "    y_2D = process_phase(file)\n",
    "    y_1D = np.mean(y_2D,axis=(1,2))\n",
    "    y_psd = temp_power_spec(y_1D,.001)\n",
    "    y_smoothed = 10**signal.savgol_filter(np.log10(y_psd), 101, 5)\n",
    "    #df_t_psd[name_list[i],'dts'] = pd.to_datetime(name_list[i],format='%Y.%m.%d_%H.%M.%S',utc=True)\n",
    "    new_df = pd.DataFrame({name_list[i]:y_smoothed})\n",
    "    df_t_psd = pd.concat([df_t_psd,new_df],axis=1)\n",
    "    print(name_list[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_t_psd.to_csv(save_path+'t_psd'+'_'+dstr+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Samples by night observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'fname_list':fname_list,'name_list':name_list})\n",
    "df['dts'] = pd.to_datetime(df['name_list'],format='%Y.%m.%d_%H.%M.%S',utc=True)\n",
    "df['delta_t'] = df['dts']-df['dts'].shift(1)\n",
    "df.loc[0,'delta_t'] = pd.Timedelta(0)\n",
    "\n",
    "def bin_f(row):\n",
    "    if row['delta_t'] > pd.Timedelta('20H'):\n",
    "        return row['dts'].date()\n",
    "    \n",
    "df['bin']=df.apply(bin_f,axis = 1)\n",
    "df.loc[0:1,'bin']=df.loc[0,'dts'].date()\n",
    "df['bin']=df['bin'].fillna(method='ffill')\n",
    "g_df = df.groupby('bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the average psd measured in each night window. Column names contain date at the start of the night "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_avg_sp_psd = pd.DataFrame()\n",
    "\n",
    "for name,group in g_df:\n",
    "    df_avg_sp_psd.loc[:,str(name)] = df_sp_psd.loc[:,group['name_list'].tolist()].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_avg_sp_psd.to_csv(save_path+'avg_sp_psd_'+dstr+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_avg_t_psd = pd.DataFrame()\n",
    "\n",
    "for name,group in g_df:\n",
    "    df_avg_t_psd.loc[:,str(name)] = df_t_psd.loc[:,group['name_list'].tolist()].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_avg_t_psd.to_csv(save_path+'avg_t_psd_'+dstr+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit power law to avg psds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_psd_results = pd.DataFrame(columns=['sp_psd_slope','sp_psd_int','sp_psd_area','t_psd_slope','t_psd_int','t_psd_area'], index = df_avg_sp_psd.columns)\n",
    "\n",
    "for i in df_avg_sp_psd.columns:\n",
    "    y = df_avg_sp_psd[i]\n",
    "    x = radialProfile(kr)\n",
    "    df_psd_results.loc[i,['sp_psd_slope','sp_psd_int']] = linear_fit(x,y,0.33,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in df_avg_sp_psd.columns:\n",
    "    y = df_avg_t_psd[i].dropna()\n",
    "    x = fft.fftfreq(len(y),.001)\n",
    "    df_psd_reseults.loc[i,['t_psd_slope','t_psd_int']] = linear_fit(x,y,2.0,30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all the CSV files and concatentae all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(data_path + \"/sp_psd_summary*.csv\")\n",
    "\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles[0:3]:\n",
    "    df = pd.read_csv(file_,index_col=0)\n",
    "    list_.append(df)\n",
    "\n",
    "frame = pd.concat(list_, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_file2 = pd.read_csv(data_path + 'sp_psd_summary20181204.csv',index_col=0) \n",
    "txt_file2_copy = copy.copy(txt_file2)\n",
    "\n",
    "sp_psd_data = pd.DataFrame(txt_file2_copy)\n",
    "sp_psd_data['dts'] = pd.to_datetime(sp_psd_data.loc[:,'filename'].str.split('_').str.get(2) + '_' + sp_psd_data.loc[:,'filename'].str.split('_').str.get(3),\n",
    "              utc=True, format = '%Y.%m.%d_%H.%M.%S')\n",
    "\n",
    "sp_psd_data['dts'] = sp_psd_data['dts'].dt.tz_localize(pytz.UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_psd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_data(data, imag = 10., tau = 1., see = 2., all_cond = True):\n",
    "    \"This selects all rows of input dataframe that satisfy conditions set by the user\"\n",
    "    \n",
    "    date1 = datetime.date(year = 2014, month = 1, day =1)\n",
    "    \n",
    "    #  AO system requirements\n",
    "    cond1 = data['dts'] > date1 \n",
    "    cond2 = data['COADDS'] == 1\n",
    "    cond3 = (data['OBSMODE'] == 'H_coron')|(data['OBSMODE'] == 'Spec')\n",
    "    cond4 = data['AOFRAMES'] == 1000\n",
    "\n",
    "    #  Good seeing conditions\n",
    "    cond5 = data['IMAG'] < imag #Bright Stars\n",
    "    cond6 = data['MASSTAU'] > tau  # slow moving turbulence [ms]\n",
    "    cond7 = data['DIMMSEE'] < see  # smaller scale turbulence [\"]\n",
    "    \n",
    "    if all_cond:\n",
    "        ind = np.where(cond1 & cond2 & cond3 & cond4 & cond5 & cond6 & cond7)[0]  \n",
    "        print len(ind)\n",
    "    else:\n",
    "        ind = np.where(cond1 & cond2 & cond3 & cond4 & cond5)[0]  \n",
    "        print len(ind)\n",
    "\n",
    "    filtered_data = data.iloc[ind]\n",
    "    #filtered_data = filtered_data.dropna(subset = ['CONTR040','cal_wfe','M1_avg','TAMBIENT','Outside_OE_temperature'])\n",
    "    \n",
    "    new_ind = np.arange(len(filtered_data))\n",
    "    filtered_data = filtered_data.set_index(new_ind)\n",
    "        \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = filter_data(raw_contrast_data,all_cond=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge PSD data with IFS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_psd_raw_IFS_data =  pd.merge_asof(sp_psd_data[['slope','dts','filename','path']].sort_values('dts'),raw_contrast_data, on='dts',tolerance=pd.Timedelta('1min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_psd_raw_IFS_data = sp_psd_raw_IFS_data.dropna(subset=['slope','M1_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print sum(np.isfinite(sp_psd_raw_IFS_data['TAMBIENT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = sp_psd_raw_IFS_data.groupby(['night_number', 'OBJNAME'], as_index= False).mean().dropna(subset = ['slope','M1_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less = np.where(np.abs(grouped['M1_avg']-grouped['T_twr'])<1.)[0]\n",
    "more = np.where(np.abs(grouped['M1_avg']-grouped['T_twr'])>=1.)[0]\n",
    "\n",
    "grouped.loc[more,'slope'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "x = grouped['M1_avg']-grouped['TAMBIENT']\n",
    "y = grouped['slope']\n",
    "print(sum(np.isfinite(y)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.scatter(x,y,c='red',marker='s')\n",
    "plt.ylabel('log of Spatial PSD Slope',fontsize=15)\n",
    "plt.xlabel('Primary - Outside air [C]',fontsize=15)\n",
    "#plt.colorbar()\n",
    "\n",
    "\n",
    "#plt.savefig(save_path+'sp_psd_slope_vs_delT_grouped_20181205.pdf')\n",
    "\n",
    "\n",
    "#plt.savefig(save_path+'sp_psd_slope_vs_delT_night_20181201.pdf')\n",
    "#plt.plot(sp_psd_raw_IFS_data['slope'],'.',alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = np.where(grouped['slope'] == np.max(grouped['slope']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_for_dome_seeing(data, sample_size, presence = True):\n",
    "    \n",
    "    cut = 0\n",
    "    sample = set()\n",
    "    \n",
    "    if presence:\n",
    "        \n",
    "        while len(sample) <= sample_size:\n",
    "            tau0 = data.MASSTAU.sort_values(ascending = False)[0:cut]\n",
    "            mirror_to_air_temp = np.abs(data.M1_avg - data.T_twr).sort_values(ascending = False)[0:cut]\n",
    "            contrast = data.CONTR040.sort_values(ascending = False)[0:cut]\n",
    "            psd_slope = data.slope.sort_values(ascending = False)[0:cut]\n",
    "            sample = set(tau0.index) & set(mirror_to_air_temp.index) & set(contrast.index) & set(psd_slope.index)\n",
    "            cut = cut + 1\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        while len(sample) <= sample_size:\n",
    "            tau0 = data.MASSTAU.sort_values(ascending = False)[0:cut]\n",
    "            mirror_to_air_temp = np.abs(data.M1_avg - data.T_twr).sort_values()[0:cut]\n",
    "            contrast = data.CONTR040.sort_values()[0:cut]\n",
    "            psd_slope = data.slope.sort_values(ascending = True)[0:cut]\n",
    "            sample = set(tau0.index) & set(mirror_to_air_temp.index) & set(contrast.index) & set(psd_slope.index)\n",
    "            cut = cut + 1\n",
    "    return list(sample)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_psd_raw_IFS_data.loc[search_for_dome_seeing(sp_psd_raw_IFS_data,10,presence=True),\n",
    "                       ['dts','CONTR040','M1_avg','TAMBIENT','MASSTAU','slope','IMAG','filename']].sort_values(\"CONTR040\",ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_psd_raw_IFS_data.loc[search_for_dome_seeing(sp_psd_raw_IFS_data,10,presence=False),\n",
    "                       ['dts','CONTR040','M1_avg','TAMBIENT','MASSTAU','slope','filename']].sort_values(\"CONTR040\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
